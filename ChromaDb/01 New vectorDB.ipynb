{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17b64f06-d49a-4760-ada8-f4fabfc0b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330fe83e-9ea5-4bbc-83c8-f8cd239f5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter, CharacterTextSplitter \n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019ae2c6-144e-4303-a6c5-cd8b618df658",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Check if variables are correctly loaded from .env\n",
    "AZURE_OPENAI_API_KEY_2 = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "if not AZURE_OPENAI_API_KEY_2:\n",
    "    raise ValueError(\"AZURE_OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "DEPLOYMENT_NAME_LLM = os.getenv('DEPLOYMENT_NAME_LLM')\n",
    "if not DEPLOYMENT_NAME_LLM:\n",
    "    raise ValueError(\"DEPLOYMENT_NAME_LLM not found in environment variables\")\n",
    "\n",
    "API_VERSION = os.getenv('API_VERSION')\n",
    "if not API_VERSION:\n",
    "    raise ValueError(\"API_VERSION not found in environment variables\")\n",
    "    \n",
    "AZURE_ENDPOINT_LLM = os.getenv('AZURE_ENDPOINT_LLM')\n",
    "if not AZURE_ENDPOINT_LLM:\n",
    "    raise ValueError(\"AZURE_ENDPOINT_LLM not found in environment variables\")\n",
    "\n",
    "EMBEDDING_KEY = os.getenv('EMBEDDING_KEY')\n",
    "if not EMBEDDING_KEY:\n",
    "    raise ValueError(\"EMBEDDING_KEY not found in environment variables\")\n",
    "\n",
    "DEPLOYMENT_NAME_EMBEDDING = os.getenv('DEPLOYMENT_NAME_EMBEDDING')\n",
    "if not DEPLOYMENT_NAME_EMBEDDING:\n",
    "    raise ValueError(\"DEPLOYMENT_NAME_EMBEDDING not found in environment variables\")\n",
    "    \n",
    "AZURE_ENDPOINT_EMBEDDING = os.getenv('AZURE_ENDPOINT_EMBEDDING')\n",
    "if not AZURE_ENDPOINT_EMBEDDING:\n",
    "    raise ValueError(\"AZURE_ENDPOINT_EMBEDDING not found in environment variables\")\n",
    "\n",
    "API_BASE_EMBEDDING = os.getenv('API_BASE_EMBEDDING')\n",
    "if not API_BASE_EMBEDDING:\n",
    "    raise ValueError(\"API_BASE_EMBEDDING not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9924c3-fd3e-4a49-a05b-827d6edba6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/A Survey of Time Series Foundation Models Generalizing Time Series.pdf\n",
      "data/DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via.pdf\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 2000\n",
    "chunk_overlap = 400\n",
    "\n",
    "# Documentos\n",
    "dict_documents = {}\n",
    "dict_documents[1] = 'A Survey of Time Series Foundation Models Generalizing Time Series.pdf'\n",
    "dict_documents[2] = 'DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via.pdf'\n",
    "\n",
    "# Seleciona arquivos\n",
    "pdf_path = {}\n",
    "pdf_path[1] = \"data/\" + dict_documents[1]\n",
    "pdf_path[2] = \"data/\" + dict_documents[2]\n",
    "\n",
    "print(pdf_path[1])\n",
    "print(pdf_path[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51813e8a-7629-4404-9588-6c492fa3952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_page_markers(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text_with_page_markers = []\n",
    "        for i in range(len(reader.pages)):\n",
    "            page = reader.pages[i].extract_text()\n",
    "            # Add page marker\n",
    "            text_with_page_markers.append(f\"[[PAGE {i + 1}]]\\n{page}\")\n",
    "    return '\\n'.join(text_with_page_markers)\n",
    "\n",
    "# Assuming text_with_page_markers is obtained from the previous step\n",
    "def chunk_text_with_page_tracking(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    chunk_page_mapping = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Find the pages in the chunk by looking for the page markers\n",
    "        start_page = None\n",
    "        end_page = None\n",
    "\n",
    "        flag_first_line = 1\n",
    "        for line in chunk.splitlines():\n",
    "            if \"[[PAGE\" in line:   # The notation '[[PAGE' originates from the splitting function, not from the document.\n",
    "                page_num = int(line.split(\"[[PAGE \")[1].split(\"]]\")[0])\n",
    "                if start_page is None:\n",
    "                    if flag_first_line == 1:\n",
    "                        start_page = page_num\n",
    "                    else:\n",
    "                        # If it starts in the middle of the page, select start_page as the previous page.\n",
    "                        start_page = page_num - 1\n",
    "                end_page = page_num\n",
    "            flag_first_line = 0\n",
    "\n",
    "        # If the string \"[[PAGE \" is not found, it means there was no page change.\n",
    "        if start_page is None:\n",
    "            start_page = end_page_aux\n",
    "            end_page = end_page_aux\n",
    "        end_page_aux = end_page\n",
    "        \n",
    "        chunk_page_mapping.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"start_page\": start_page,\n",
    "            \"end_page\": end_page\n",
    "        })\n",
    "        \n",
    "    return chunk_page_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21672d34-f1ad-4bbb-a645-220f719ce0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "pdf_chunked_data = {}\n",
    "for key in tqdm.tqdm(pdf_path):\n",
    "    text_with_pages = extract_text_with_page_markers(pdf_path[key])\n",
    "    pdf_chunked_data[key] = chunk_text_with_page_tracking(text_with_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203df37c-03cd-4f65-af83-2a00bb9a7ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line, we discuss effectiveness through two key phases: data collection and alignment, architectural design. Regarding the\n",
      "second line, we identify two adaption paradigms, i.e. embedding visible LLM adaption and textual visible LLM adaption.\n",
      "Under each adaption paradigm, we discuss the LLM utilization, time series extraction and multi-modal data fusion. The\n",
      "time series extraction includes challenges like obtaining appropriate time series representation, aligning temporal space\n",
      "and LLM space, identifying time series properties and patterns. Additionally, we examine diverse roles of LLMs that\n",
      "[[PAGE 5]]\n",
      "A Survey of Time Series Foundation Models 5\n",
      "SurveyEffectiveness Efficiency Explainability Domain\n",
      "Foundation Model Pre-trained from Scratch\n",
      "for Time SeriesLLM Adaption for Time Series\n",
      "Efficient\n",
      "TuningLocal\n",
      "ExplanationGlobal\n",
      "ExplanationSpecific or\n",
      "GeneralAdaption to\n",
      "Time SeriesAlignmentTime Series\n",
      "CharacteristicsMultimodal\n",
      "[84] ✗ ✓ ✓ ✗ ✓ ✗ ✗ ✗ Specific\n",
      "[83] ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✗ Both\n",
      "[154] ✗ ✓ ✓ ✓ ✗ ✗ ✗ ✗ General\n",
      "[81] ✗ ✓ ✓ ✓ ✓ ✓ ✗ ✗ Both\n",
      "[205] ✓ ✓ ✓ ✗ ✓ ✗ ✗ ✗ Both\n",
      "Ours ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Both\n",
      "Table 1. Comparative overview of related surveys, spanning four aspects: Effectiveness, Efficiency, Explainability, and\n",
      "Domain.\n",
      "further increase the effectiveness of LLM adaption. (2) How to pre-train or fine-tune foundation models efficiently for\n",
      "time series tasks? Given that this area is emerging, current efficient techniques are adapted from NLP. Therefore, we\n",
      "first provide a brief overview of cutting-edge NLP efficient methods that are transferable to this context. We then discuss\n",
      "the efficiency under different tuning paradigms and summarize efficient methods already in use. (3) How to obtain\n",
      "explainability of foundation models’ behaviors or decisions in time series applications? The practical deployment of\n",
      "models necessitates explainability. We start by exploring the concept of explainability in AI, highlighting both global and\n"
     ]
    }
   ],
   "source": [
    "# Show chunk_page_mapping example. \n",
    "print(pdf_chunked_data[1][10]['chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128b3d2e-edd1-4c5b-af21-04322600777b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunked_data[1][10]['start_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f32a1b1-203e-4b40-98ee-3660af04229c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_chunked_data[2][0]['end_page']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91d809-19d8-464e-a2f4-d73a95715aff",
   "metadata": {},
   "source": [
    "## question-augmented vector retrieval (QAVR)\n",
    "\n",
    "A dual-vector storage approach for contextual augmentation.\n",
    "\n",
    "- Collection for Texts: Storing the original text chunks in one collection for direct semantic retrieval.\n",
    "- Collection for Hypothetical Questions: Creating another collection with hypothetical questions that each text chunk could answer. This enhances retrieval by matching user queries with questions semantically similar to their intent, rather than directly to the text.\n",
    "\n",
    "Related Concepts:\n",
    "- Augmented Retrieval: Augmenting the dataset with additional metadata, in this case, hypothetical questions.\n",
    "- Embedding-based Retrieval with Intent Mapping: Mapping potential user intents (questions) to the text that best answers them.\n",
    "- Query Expansion: While query expansion typically involves modifying the user's query, your approach effectively expands the dataset to cover a broader range of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b119f07-be3e-4c0d-9ecc-b4f1a6086f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model used\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name = DEPLOYMENT_NAME_LLM,\n",
    "    model_name = \"gpt-4o-mini\",\n",
    "    api_version = API_VERSION,\n",
    "    azure_endpoint = AZURE_ENDPOINT_LLM,\n",
    "    api_key = AZURE_OPENAI_API_KEY_2,\n",
    ")\n",
    "# Embedding for LangChain\n",
    "embedding_function_to_langchain = AzureOpenAIEmbeddings(\n",
    "    model = 'text-embedding-3-small'\n",
    "    , deployment = DEPLOYMENT_NAME_EMBEDDING\n",
    "    , azure_endpoint = AZURE_ENDPOINT_EMBEDDING\n",
    ") \n",
    "\n",
    "# Embedding for ChromaDB\n",
    "embedding_function = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key = EMBEDDING_KEY,\n",
    "                api_base = API_BASE_EMBEDDING ,\n",
    "                api_type = \"azure\",\n",
    "                api_version=\"2023-05-15\",\n",
    "                model_name=DEPLOYMENT_NAME_EMBEDDING\n",
    "            )\n",
    "\n",
    "# Model used for the Collection for Hypothetical Questions\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "{context}\n",
    "If you can't make a answer with context, just say that you don't know, don't try to make up an answer.\n",
    "Do not hallucinate.\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "question = \"\"\"Make as many relevant specific and/or generic questions that the above text can answer.\n",
    "If you can't make a question with context, just don't say anything, don't try to make up an questions just to fill the quota.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fdb00-e1d2-48ed-b06b-fcdf533becd3",
   "metadata": {},
   "source": [
    "## Create VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ec06df-921d-4065-a787-facbf678bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local Croma database.\n",
    "persist_directory = 'embedding_OpenAI/chroma/'\n",
    "client = chromadb.PersistentClient(path = persist_directory)\n",
    "# Create or get a collection\n",
    "collection_full_text = client.get_or_create_collection(name = \"full_text\", embedding_function = embedding_function)\n",
    "collection_questions_text = client.get_or_create_collection(name = \"questions_text\", embedding_function = embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e14a567-3a68-487c-8f7e-40a313434d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello.\n",
      "Who am I?\n",
      "I'm just a test to verify it this documment was correctly included in the vectorDB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New text\n",
    "# Agent's introdutory text\n",
    "text_who_am_I = \"\"\"\n",
    "Hello.\n",
    "Who am I?\n",
    "I'm just a test to verify it this documment was correctly included in the vectorDB.\n",
    "\"\"\"\n",
    "print(text_who_am_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a6b941-96de-419a-9b42-08296d871f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [04:45<00:00,  2.60s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [01:30<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add new text in the collection\n",
    "add_text_collection_full_text = 1\n",
    "if add_text_collection_full_text == 1:\n",
    "    vector_db_id = 1\n",
    "    document_name = \"Agent_description\"\n",
    "    collection_full_text.add(\n",
    "        documents = [text_who_am_I], # The method .add() in the Chroma collections expects a list of documents. You have provided a single string (text_who_am_I). Make sure it’s wrapped in a list.\n",
    "        metadatas = {\"document_name\": document_name,  \n",
    "            \"vector_db_id\": vector_db_id, \n",
    "            'first_page': -1, # This is a custom added text, so there isn't a first and last page.\n",
    "            'last_page': -1,\n",
    "            'added_by': 'default',\n",
    "        },\n",
    "        ids = [str(vector_db_id)]\n",
    "    )\n",
    "    # Make questions\n",
    "    context = text_who_am_I\n",
    "    result = llm_chain.invoke({\"context\": context, \"question\": question})\n",
    "    # Add questions in the collection\n",
    "    collection_questions_text.add(\n",
    "        documents = result.content,\n",
    "        metadatas = {\"document_name\": document_name, \n",
    "            \"vector_db_id\": vector_db_id, \n",
    "            'first_page': -1, \n",
    "            'last_page': -1,\n",
    "            'added_by': 'default',\n",
    "        },\n",
    "        ids = [str(vector_db_id)]\n",
    "    )\n",
    "    # Add documents in dict_documents\n",
    "    vector_db_id += 1\n",
    "    for key in pdf_chunked_data:\n",
    "        document_name = dict_documents[key]\n",
    "        for e in tqdm.tqdm(pdf_chunked_data[key]):\n",
    "            context = e['chunk']\n",
    "            collection_full_text.add(\n",
    "                documents = context,\n",
    "                metadatas = {\"document_name\": document_name,\n",
    "                    \"vector_db_id\": vector_db_id,\n",
    "                    'first_page': e['start_page'],\n",
    "                    'last_page': e['end_page'],\n",
    "                    'added_by': 'default',\n",
    "                },\n",
    "                ids = [str(vector_db_id)]\n",
    "            )\n",
    "            # Make questions\n",
    "            result = llm_chain.invoke({\"context\": context, \"question\": question})\n",
    "            collection_questions_text.add(\n",
    "                documents = result.content,\n",
    "                metadatas = {\"document_name\": document_name,\n",
    "                    \"vector_db_id\": vector_db_id,\n",
    "                    'first_page': e['start_page'],\n",
    "                    'last_page': e['end_page'],\n",
    "                    'added_by': 'default',\n",
    "                },\n",
    "                ids = [str(vector_db_id)]\n",
    "            )\n",
    "            vector_db_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25dd9ca-3764-4463-9e44-b463346ee474",
   "metadata": {},
   "source": [
    "## Add customized document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e546fc1-ed3a-4269-8972-3b56087741e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "person_name = \"Max Wienandts\"\n",
    "document_name = \"Custon added\"\n",
    "first_page = -1\n",
    "last_page = -1\n",
    "\n",
    "text_to_add = \"\"\"\n",
    "This is a brand new text.\n",
    "The correct number to choose is 42.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "954e55f3-5abd-49c4-9c83-c19928d2fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest index: 147\n"
     ]
    }
   ],
   "source": [
    "# Chunk new text\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "chunks = text_splitter.split_text(text_to_add)\n",
    "\n",
    "# Get last index\n",
    "# Get last id\n",
    "# Retrieve documents with metadata, which may contain IDs\n",
    "documents = collection_full_text.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "# Assuming your IDs are stored in the metadata and are numeric or sortable\n",
    "max_index = max([doc['vector_db_id'] for doc in documents['metadatas']], key=lambda x: int(x))\n",
    "\n",
    "print(f\"Largest index: {max_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a9af2c6-7441-4e23-b61f-d80b8b1fb3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add new text in the full text collection\n",
    "add_text_collection_full_text = 1\n",
    "if add_text_collection_full_text == 1:\n",
    "    vector_db_id = max_index + 1\n",
    "    for e in tqdm.tqdm(chunks):\n",
    "        collection_full_text.add(\n",
    "            documents = e,\n",
    "            metadatas = {\"document_name\": document_name,\n",
    "                \"vector_db_id\": vector_db_id,\n",
    "                'first_page': first_page,\n",
    "                'last_page': last_page,\n",
    "                'person_name': person_name,\n",
    "            },\n",
    "            ids = [str(vector_db_id)]\n",
    "        )\n",
    "        # Add new text in the question collection\n",
    "        context = e\n",
    "        result = llm_chain.invoke({\"context\": context, \"question\": question})\n",
    "        collection_questions_text.add(\n",
    "            documents = result.content,\n",
    "            metadatas = {\"document_name\": document_name,\n",
    "                \"vector_db_id\": vector_db_id,\n",
    "                'first_page': first_page,\n",
    "                'last_page': last_page,\n",
    "                'person_name': person_name,\n",
    "            },\n",
    "            ids = [str(vector_db_id)]\n",
    "        )\n",
    "\n",
    "        vector_db_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657758d-6f57-4385-8687-3b4a4a0cbacb",
   "metadata": {},
   "source": [
    "## Verify if collections have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d467c315-3b18-49d1-aac9-a0b6e9fbb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text largest index: 148\n",
      "Questions largest index: 148\n"
     ]
    }
   ],
   "source": [
    "# Create or get a collection\n",
    "collection_full_text_aux = client.get_or_create_collection(name = \"full_text\", embedding_function = embedding_function)\n",
    "collection_questions_aux = client.get_or_create_collection(name = \"questions_text\", embedding_function = embedding_function)\n",
    "\n",
    "# Get last index\n",
    "# Get last id\n",
    "# Retrieve documents with metadata, which may contain IDs\n",
    "documents_full_text = collection_full_text_aux.get(include=[\"documents\", \"metadatas\"])\n",
    "documents_questions_text = collection_questions_aux.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "# Assuming your IDs are stored in the metadata and are numeric or sortable\n",
    "max_index_full_text = max([doc['vector_db_id'] for doc in documents_full_text['metadatas']], key=lambda x: int(x))\n",
    "max_index_questions = max([doc['vector_db_id'] for doc in documents_questions_text['metadatas']], key=lambda x: int(x))\n",
    "\n",
    "\n",
    "print(f\"Full text largest index: {max_index_full_text}\")\n",
    "print(f\"Questions largest index: {max_index_questions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
